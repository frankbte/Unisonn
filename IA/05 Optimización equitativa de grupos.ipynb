{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2692120-732b-4336-9969-500a32f73b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "import ipywidgets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "plt.ioff();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a2534b-c4aa-42a6-99c5-e6ef1752592a",
   "metadata": {},
   "source": [
    "# Regresión lineal\n",
    "\n",
    "Consideremos los siguientes datos de entrenamiento en un problema de regresión lineal con modelos simples uniparamétricos y pérdida cuadrática.\n",
    "\n",
    "| $x$ | $y$ |\n",
    "|-----|-----|\n",
    "|  1  |  4  |\n",
    "|  2  |  8  |\n",
    "|  5  |  5  |\n",
    "|  6  |  6  |\n",
    "|  7  |  7  |\n",
    "|  8  |  8  |\n",
    "\n",
    "$$\\mathcal{F} = \\left\\{ f_\\mathbf{w} \\mid \\mathbf{w}\\in\\mathbb{R}, f_\\mathbf{w}(x) = \\mathbf{w}\\cdot\\phi(x), \\phi(x) = x\\right\\}$$\n",
    "\n",
    "$$\\text{Loss}(x, y, \\mathbf{w}) = (f_\\mathbf{w}(x) - y)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eabc5c2-3649-417d-a41d-f281e7761773",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dtrain = [\n",
    "    (1, 4),\n",
    "    (2, 8),\n",
    "    (5, 5),\n",
    "    (6, 6),\n",
    "    (7, 7),\n",
    "    (8, 8),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3959e7fc-7ad3-4608-a506-978e7e518132",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67cdaea-cb9b-4aab-8609-606b7444c1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, x):\n",
    "    return w * features(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9c8113-5afa-4ddf-bd25-634a0195a859",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(x, y, w):\n",
    "    return (predict(w, x) - y) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d03c6a-d37e-40c4-ab3d-d0e63ea035e1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plt.close()\n",
    "\n",
    "xmin, xmax = 0, 8\n",
    "ymin, ymax = 0, 8\n",
    "\n",
    "w_init, w_min, w_max, w_step = 0.5, 0.0, 20.0, 0.01\n",
    "\n",
    "def plot_Dtrain(ax):\n",
    "    xs, ys = [], []\n",
    "    for x, y in Dtrain:\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return ax.scatter(xs, ys, c = \"black\")\n",
    "\n",
    "def plot_predictor(ax):\n",
    "    xs = [xmin, xmax]\n",
    "    ys = [predict(w_init, x) for x in xs]\n",
    "    return ax.plot(xs, ys, c = \"black\")\n",
    "\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "ax = fig.add_subplot()\n",
    "plot_Dtrain(ax)\n",
    "lines = plot_predictor(ax)\n",
    "ax.set_xlabel(\"$x$\", size=20)\n",
    "ax.set_xlim((xmin, xmax + 1))\n",
    "ax.set_ylabel(\"$y$\", size=20)\n",
    "ax.set_ylim((ymin, ymax + 1))\n",
    "fig.canvas.header_visible = False\n",
    "\n",
    "def update_plot(w):\n",
    "    xs = ax.get_xlim()\n",
    "    ys = [predict(w, x) for x in xs]\n",
    "    lines[0].set_data(xs, ys)\n",
    "    fig.canvas.draw()\n",
    "    fig.canvas.flush_events()\n",
    "\n",
    "widget = ipywidgets.interactive(\n",
    "    update_plot,\n",
    "    w = ipywidgets.FloatSlider(\n",
    "        orientation = \"horizontal\",\n",
    "        description = \"w\",\n",
    "              value = w_init,\n",
    "                min = w_min,\n",
    "                max = w_max,\n",
    "               step = w_step,\n",
    "             layout = ipywidgets.Layout(width='90%')\n",
    "    ),\n",
    ")\n",
    "\n",
    "display(widget)\n",
    "display(fig.canvas);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b976235-537e-4131-9e3b-8d136a475129",
   "metadata": {},
   "source": [
    "## Pérdida promedio\n",
    "\n",
    "En los problemas de regresión, utilizamos la pérdida promedio para plantear un problema de optimización que nos permita encontrar el mejor predictor utilizando descenso de gradiente.\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\text{TrainLoss}(\\mathbf{w}) &= \\frac{1}{|\\mathcal{D}_\\text{train}|} \\sum_{(x,y)\\in\\mathcal{D}_\\text{train}} \\text{Loss}(x, y, \\mathbf{w}) \\\\\n",
    "\\nabla_\\mathbf{w} \\text{TrainLoss}(\\mathbf{w}) &= \\frac{1}{|\\mathcal{D}_\\text{train}|} \\sum_{(x,y)\\in\\mathcal{D}_\\text{train}} \\nabla_\\mathbf{w}\\text{Loss}(x, y, \\mathbf{w}) \\\\\n",
    "&= \\frac{1}{|\\mathcal{D}_\\text{train}|} \\sum_{(x,y)\\in\\mathcal{D}_\\text{train}} \\nabla_\\mathbf{w}(f_\\mathbf{w}(x) - y)^2 \\\\\n",
    "&= \\frac{1}{|\\mathcal{D}_\\text{train}|} \\sum_{(x,y)\\in\\mathcal{D}_\\text{train}} 2(f_\\mathbf{w}(x) - y)\\phi(x)\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0b5288-4b86-4451-ac3f-7c2eb15bfb9b",
   "metadata": {},
   "source": [
    "Aunque también podemos encontrar el valor de $\\mathbf{w}$ que minimiza la pérdida de entrenamiento analíticamente...\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\nabla_\\mathbf{w}\\text{TrainLoss}(\\mathbf{w}) &= \\frac{1}{6}\\sum_{(x,y)\\in\\mathcal{D}_\\text{train}} 2(\\mathbf{w}\\cdot\\phi(x)-y)\\phi(x) \\\\\n",
    "&= \\frac{1}{6}\\sum_{(x,y)\\in\\mathcal{D}_\\text{train}} 2(\\mathbf{w}\\cdot x-y)x \\\\\n",
    "&= \\frac{2}{6}\\sum_{(x,y)\\in\\mathcal{D}_\\text{train}} (\\mathbf{w}\\cdot x-y)x \\\\\n",
    "&= \\frac{1}{3}\\left[ (\\mathbf{w}\\cdot 1-4)1 + (\\mathbf{w}\\cdot 2-8)2 + (\\mathbf{w}\\cdot 5-5)5 + (\\mathbf{w}\\cdot 6-6)6 + (\\mathbf{w}\\cdot 7-7)7 + (\\mathbf{w}\\cdot 8-8)8  \\right] \\\\\n",
    "&= \\frac{1}{3}\\left[ (\\mathbf{w}-4) + (4\\mathbf{w}-16) + (25\\mathbf{w}-25) + (36\\mathbf{w}-36) + (49\\mathbf{w}-49) + (64\\mathbf{w}-64)  \\right] \\\\\n",
    "&= \\frac{1}{3}\\left[ \\mathbf{w}-4 + 4\\mathbf{w}-16 + 25\\mathbf{w}-25 + 36\\mathbf{w}-36 + 49\\mathbf{w}-49 + 64\\mathbf{w}-64  \\right] \\\\\n",
    "&= \\frac{1}{3}\\left[ (\\mathbf{w} + 4\\mathbf{w} + 25\\mathbf{w} + 36\\mathbf{w} + 49\\mathbf{w} + 64\\mathbf{w}) + (-4-16-25-36-49-64)  \\right] \\\\\n",
    "&= \\frac{1}{3}\\left[ 179\\mathbf{w}-194  \\right] \\\\\n",
    "&= \\frac{179}{3}\\mathbf{w}-\\frac{194}{3}\n",
    "\\end{aligned}$$\n",
    "\n",
    "Encontramos el peso $\\mathbf{w}_\\min$ que minimiza $\\text{TrainLoss}$ encontrando la raíz del gradiente de $\\text{TrainLoss}$.\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\nabla_\\mathbf{w}\\text{TrainLoss}(\\mathbf{w_\\min}) &= 0 \\\\\n",
    "\\frac{179}{3}\\mathbf{w_\\min}-\\frac{194}{3} &= 0 \\\\\n",
    "\\frac{179}{3}\\mathbf{w_\\min} &= \\frac{194}{3} \\\\\n",
    "\\mathbf{w_\\min} &= \\frac{194\\cdot3}{3\\cdot179} \\\\\n",
    "                &= \\frac{582}{537} \\\\\n",
    "                &= 1.083798882681564245810055866\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08705ab-de71-4e77-a280-5f4d867db071",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loss(Dtrain, w):\n",
    "    examples = len(Dtrain)\n",
    "    total = sum(loss(x, y, w) for x, y in Dtrain)\n",
    "    return total / examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baf9572-283b-42ea-86c9-35e910add9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_train_loss(Dtrain, w):\n",
    "    examples = len(Dtrain)\n",
    "    total = sum(2*(predict(w, x)-y)*features(x) for x, y in Dtrain)\n",
    "    return total / examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b68743-b184-4bf4-a9d0-380330e66320",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(Dtrain, eta, T):\n",
    "    w = 0.0\n",
    "    for t in range(1, T + 1):\n",
    "        tl = train_loss(Dtrain, w)\n",
    "        gtl = grad_train_loss(Dtrain, w)\n",
    "        w = w - eta * gtl\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5838e028-e832-42e7-9d28-7b3d8657af33",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_best = gradient_descent(Dtrain, 0.01, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae469e6-af86-4d59-9826-d08a259163c3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plt.close()\n",
    "\n",
    "w_min, w_max, w_step = 0.0, 5.0, 0.1\n",
    "l_min, l_max = 0, 80\n",
    "\n",
    "fig = plt.figure();\n",
    "ax = fig.add_subplot()\n",
    "xs = np.arange(w_min, w_max, w_step)\n",
    "ys = [train_loss(Dtrain, w) for w in xs]\n",
    "ax.plot(xs, ys, c = \"black\", label = \"$\\\\text{TrainLoss}$\")\n",
    "ax.scatter([w_best], [train_loss(Dtrain, w_best)], c = \"black\", s = 30, label = \"$\\\\mathbf{w}_\\\\min =$\" + str(w_best)[:5])\n",
    "ax.set_xlabel(\"$\\\\mathbf{w}$\")\n",
    "ax.set_xlim((w_min, w_max))\n",
    "ax.set_ylabel(\"$\\\\text{loss}$\")\n",
    "ax.set_ylim((l_min, l_max))\n",
    "ax.legend()\n",
    "fig.canvas.header_visible = False\n",
    "\n",
    "display(fig.canvas);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f121d2-b105-4de3-9073-71edf8925bb6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plt.close()\n",
    "\n",
    "xmin, xmax = 0, 8\n",
    "ymin, ymax = 0, 8\n",
    "\n",
    "w_init, w_min, w_max, w_step = w_best, 0.0, 20.0, 0.01\n",
    "\n",
    "def plot_Dtrain(ax):\n",
    "    xs, ys = [], []\n",
    "    for x, y in Dtrain:\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return ax.scatter(xs, ys, c = \"black\")\n",
    "\n",
    "def plot_predictor(ax):\n",
    "    xs = [xmin, xmax]\n",
    "    ys = [predict(w_init, x) for x in xs]\n",
    "    return ax.plot(xs, ys, c = \"black\")\n",
    "    \n",
    "\n",
    "plt.ioff()\n",
    "\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "ax = fig.add_subplot()\n",
    "plot_Dtrain(ax)\n",
    "lines = plot_predictor(ax)\n",
    "ax.set_xlabel(\"$x$\", size=20)\n",
    "ax.set_xlim((xmin, xmax + 1))\n",
    "ax.set_ylabel(\"$y$\", size=20)\n",
    "ax.set_ylim((ymin, ymax + 1))\n",
    "fig.canvas.header_visible = False\n",
    "\n",
    "display(fig.canvas);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463c7ae8-6c56-4f76-8175-189d2a4378ee",
   "metadata": {},
   "source": [
    "¡Perfecto! Hemos encontrado el mejor predictor para los datos\n",
    "\n",
    "\n",
    "<img style=\"width:100%; padding-top:10rem\" src=\"https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExa21ydGw3cWFuZzd4a3Ztd2hxMGEwc3gwazgycHQ0bnp3ODRxb29uZiZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/NVEtOZRkhukZVBB32W/giphy.gif\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21362b4d-be2d-4e99-bb3f-6aa28b488402",
   "metadata": {},
   "source": [
    "Los datos presentan dos comportamientos lineales diferentes.\n",
    "\n",
    "Un comportamiento lineal corresponde a una minoría de los ejemplos y el otro a una mayoría.\n",
    "\n",
    "Se han encontrado este tipo de discrepancias en datos de entrenamiento utilizados en la vida real.\n",
    "\n",
    "Puedes conocer más al respecto en el proyecto [Gender Shades](http://gendershades.org/).\n",
    "\n",
    "Los modelos de aprendizaje máquina, de ser entrenados y utilizados ingenuamente, pueden desfavorecer a ciertos grupos, usualmente minorías. También esto ocurre al incorporar pocos datos de entrenamiento para clases protegidas ya sea en cuestión de sexo, género, etnicidad, estatus socioeconómico, ubicación geográfica, entre otras.  Las predicciones realizadas y el uso que les damos los humanos tienen [serias consecuencias](https://youtu.be/qOEtQYGwZ8w?si=CLka51Wsg05Rjglx)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b88ee8-2571-4d2f-84a3-c7447e60b129",
   "metadata": {},
   "source": [
    "# Regresión lineal con grupos\n",
    "\n",
    "\n",
    "Identifiquemos los dos comportamientos lineales con una tercer columna de grupo.\n",
    "\n",
    "| $x$ | $y$ | $g$ |\n",
    "|-----|-----|-----|\n",
    "|  1  |  4  |  A  |\n",
    "|  2  |  8  |  A  |\n",
    "|  5  |  5  |  B  |\n",
    "|  6  |  6  |  B  |\n",
    "|  7  |  7  |  B  |\n",
    "|  8  |  8  |  B  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b48c86-a809-4009-ad87-8acb2bc78752",
   "metadata": {},
   "outputs": [],
   "source": [
    "DtrainG = [\n",
    "    (1, 4, 'A'),\n",
    "    (2, 8, 'A'),\n",
    "    (5, 5, 'B'),\n",
    "    (6, 6, 'B'),\n",
    "    (7, 7, 'B'),\n",
    "    (8, 8, 'B'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e8c9c1-7470-465f-8acb-85e5879d9e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Dtrain_groups(Dtrain):\n",
    "    return set(g for _, _, g in Dtrain)\n",
    "\n",
    "def DtrainGroup(Dtrain, group):\n",
    "    return [(x, y) for (x, y, g) in Dtrain if group == g]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c034038f-0ff9-4c73-ad3f-c91b1dec034c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plt.close()\n",
    "\n",
    "xmin, xmax = 0, 8\n",
    "ymin, ymax = 0, 8\n",
    "\n",
    "w_init, w_min, w_max, w_step = w_best, 0.0, 20.0, 0.01\n",
    "\n",
    "def plot_Dtrain(ax):\n",
    "    xs1, ys1, xs2, ys2 = [], [], [], []\n",
    "    for x, y, g in DtrainG:\n",
    "        if g == \"A\":\n",
    "            xs1.append(x)\n",
    "            ys1.append(y)\n",
    "        else:\n",
    "            xs2.append(x)\n",
    "            ys2.append(y)\n",
    "    return [\n",
    "        ax.scatter(xs1, ys1, c = \"orange\", label=\"A\"),\n",
    "        ax.scatter(xs2, ys2, c = \"purple\", label=\"B\"),\n",
    "    ]\n",
    "\n",
    "def plot_predictor(ax):\n",
    "    xs = [xmin, xmax]\n",
    "    ys = [predict(w_init, x) for x in xs]\n",
    "    return ax.plot(xs, ys, c = \"black\")\n",
    "    \n",
    "\n",
    "plt.ioff()\n",
    "\n",
    "fig = plt.figure(figsize=(5,5));\n",
    "ax = plt.axes();\n",
    "plot_Dtrain(ax)\n",
    "lines = plot_predictor(ax)\n",
    "ax.set_xlabel(\"$x$\", size=20)\n",
    "ax.set_xlim((xmin, xmax + 1))\n",
    "ax.set_ylabel(\"$y$\", size=20)\n",
    "ax.set_ylim((ymin, ymax + 1))\n",
    "ax.legend()\n",
    "fig.canvas.header_visible = False\n",
    "\n",
    "display(fig.canvas);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef987f5-bf48-4a32-a4e7-7c283864ef5f",
   "metadata": {},
   "source": [
    "¿A qué grupo favorece más el “mejor” predictor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d961cf9-58b6-4dc6-a49e-9aa24e61e500",
   "metadata": {},
   "source": [
    "## Pérdida por grupo\n",
    "\n",
    "Consideremos la misma pérdida cuadrada, pero ahora tomamos la pérdida de entrenamiento por cada grupo.\n",
    "\n",
    "Veamos la pérdida de entrenamiento promedio por cada grupo..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fa76f1-cf96-470b-a8b8-3c0c049bd6f7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plt.close()\n",
    "\n",
    "w_min, w_max, w_step = 0.0, 5.0, 0.1\n",
    "l_min, l_max = 0, 80\n",
    "\n",
    "fig = plt.figure();\n",
    "ax = fig.add_subplot()\n",
    "xs = np.arange(w_min, w_max, w_step)\n",
    "ys1 = [train_loss(DtrainGroup(DtrainG, \"A\"), w) for w in xs]\n",
    "ys2 = [train_loss(DtrainGroup(DtrainG, \"B\"), w) for w in xs]\n",
    "ax.plot(xs, ys1, c = \"orange\", label = \"$\\\\text{TrainLoss}_A$\")\n",
    "ax.plot(xs, ys2, c = \"purple\", label = \"$\\\\text{TrainLoss}_B$\")\n",
    "ax.set_xlabel(\"$\\\\mathbf{w}$\")\n",
    "ax.set_xlim((w_min, w_max))\n",
    "ax.set_ylabel(\"$\\\\text{loss}$\")\n",
    "ax.set_ylim((l_min, l_max))\n",
    "ax.legend()\n",
    "fig.canvas.header_visible = False\n",
    "\n",
    "display(fig.canvas);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb903ebc-b1d4-4618-a933-a3a1bd8caa5a",
   "metadata": {},
   "source": [
    "Si usamos la $\\mathbf{w}_\\min$ calculada previamente, la pérdida de entrenamiento promedio para el grupo A es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6955fc-f8f2-4589-85cc-fa707d6a713f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss(DtrainGroup(DtrainG, \"A\"), w_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6834f4-5c99-44c8-93d9-d29f6802ca94",
   "metadata": {},
   "source": [
    "Mientras que la pérdida de entrenamiento promedio para el grupo B es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cd5d6e-45cb-48b5-a53f-6f9508d41cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss(DtrainGroup(DtrainG, \"B\"), w_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cc13d9-0b96-4ddd-8c40-6f2db09e4ba9",
   "metadata": {},
   "source": [
    "Aunque la pérdida de entrenamiento promedio sobre todo el conjunto de entrenamiento fue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13011330-3450-4345-b285-1c975458bc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss(Dtrain, w_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5be9a2-e282-483f-9c39-d6b01c8c8b9b",
   "metadata": {},
   "source": [
    "hay una disparidad muy grande en el rendimiento entre los grupos, donde el grupo A es el más afectado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135149f8-244d-4c76-bf10-697a32b982e4",
   "metadata": {},
   "source": [
    "## Pérdida máxima de grupo\n",
    "\n",
    "Queremos capturar las pérdidas por grupo con un solo valor. Para hacer esto vamos a considerar el peor caso sobre todos los grupos.\n",
    "\n",
    "Definimos la **pérdida máxima de grupo** como la pérdida máxima sobre todos cada grupo $g$\n",
    "\n",
    "$$\\text{TrainLoss}_\\max(\\mathbf{w}) = \\max_g \\text{TrainLoss}_g(\\mathbf{w})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2733bcb8-34a6-45be-9e58-4f5f93df6675",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loss_max(Dtrain, w):\n",
    "    gs = Dtrain_groups(Dtrain)\n",
    "    return max(\n",
    "        ((train_loss(DtrainGroup(Dtrain, g), w), g) for g in gs),\n",
    "        key = lambda p: p[0],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad597503-8dce-4aac-9f9a-4b52b996320f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_max(DtrainG, w_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebfff8d-7888-4527-a2f2-811ab2c11033",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_train_loss_max(Dtrain, w):\n",
    "    tl, g = train_loss_max(Dtrain, w)\n",
    "    return grad_train_loss(DtrainGroup(Dtrain, g), w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150a8505-3d84-49f3-98d2-7f39a9d5e206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_max(Dtrain, eta, T):\n",
    "    w = 0.0\n",
    "    for t in range(1, T + 1):\n",
    "        gtl = grad_train_loss_max(Dtrain, w)\n",
    "        w = w - eta * gtl\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1681462e-3ef6-4474-a0f2-6aa8e605564b",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_gbest = gradient_descent_max(DtrainG, 0.001, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81509040-b20a-46c6-82b7-3ace233264d6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plt.close()\n",
    "\n",
    "w_min, w_max, w_step = 0.0, 5.0, 0.01\n",
    "l_min, l_max = -1, 80\n",
    "\n",
    "fig = plt.figure(figsize=(8,5));\n",
    "ax = fig.add_subplot()\n",
    "xs = np.arange(w_min, w_max, w_step)\n",
    "ys0 = [train_loss(Dtrain, w) for w in xs]\n",
    "ys1 = [train_loss(DtrainGroup(DtrainG, \"A\"), w) for w in xs]\n",
    "ys2 = [train_loss(DtrainGroup(DtrainG, \"B\"), w) for w in xs]\n",
    "ys3 = [train_loss_max(DtrainG, w)[0] for w in xs]\n",
    "ax.plot(xs, ys0, c = \"black\", label = \"$\\\\text{TrainLoss}$\")\n",
    "ax.plot(xs, ys1, c = \"orange\", label = \"$\\\\text{TrainLoss}_A$\")\n",
    "ax.plot(xs, ys2, c = \"purple\", label = \"$\\\\text{TrainLoss}_B$\")\n",
    "ax.plot(xs, ys3, c = \"blue\", label = \"$\\\\text{TrainLoss}_\\\\max$\")\n",
    "\n",
    "w_best = gradient_descent(Dtrain, 0.01, 1000)\n",
    "ax.scatter([w_best], [train_loss(Dtrain, w_best)],\n",
    "           c = \"black\", s = 30, label = \"$\\\\mathbf{w}_\\\\min =$\" + str(round(w_best, 3)))\n",
    "\n",
    "w_A = gradient_descent(DtrainGroup(DtrainG, \"A\"), 0.01, 1000)\n",
    "ax.scatter([w_A], [train_loss(DtrainGroup(DtrainG, \"A\"), w_A)],\n",
    "           c = \"orange\", s = 30, label = \"$\\\\mathbf{w}_A =$\" + str(round(w_A, 3)))\n",
    "\n",
    "w_B = gradient_descent(DtrainGroup(DtrainG, \"B\"), 0.01, 1000)\n",
    "ax.scatter([w_B], [train_loss(DtrainGroup(DtrainG, \"B\"), w_B)],\n",
    "           c = \"purple\", s = 30, label = \"$\\\\mathbf{w}_B =$\" + str(round(w_B, 3)))\n",
    "\n",
    "w_gbest = gradient_descent_max(DtrainG, 0.001, 1000)\n",
    "ax.scatter([w_gbest], [train_loss_max(DtrainG, w_gbest)[0]],\n",
    "           c = \"blue\", s = 30, label = \"$\\\\mathbf{w}_{g\\\\min} =$\" + str(round(w_gbest, 3)))\n",
    "\n",
    "ax.set_xlabel(\"$\\\\mathbf{w}$\")\n",
    "ax.set_xlim((w_min, w_max))\n",
    "ax.set_ylabel(\"$\\\\text{loss}$\")\n",
    "ax.set_ylim((l_min, l_max))\n",
    "ax.legend()\n",
    "fig.canvas.header_visible = False\n",
    "\n",
    "display(fig.canvas);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235387ed-c080-444e-9580-f4eedfc595a9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plt.close()\n",
    "\n",
    "xmin, xmax = 0, 8\n",
    "ymin, ymax = 0, 8\n",
    "\n",
    "def plot_Dtrain(ax):\n",
    "    xs1, ys1, xs2, ys2 = [], [], [], []\n",
    "    for x, y, g in DtrainG:\n",
    "        if g == \"A\":\n",
    "            xs1.append(x)\n",
    "            ys1.append(y)\n",
    "        else:\n",
    "            xs2.append(x)\n",
    "            ys2.append(y)\n",
    "    return [\n",
    "        ax.scatter(xs1, ys1, c = \"orange\", label=\"A\"),\n",
    "        ax.scatter(xs2, ys2, c = \"purple\", label=\"B\"),\n",
    "    ]\n",
    "\n",
    "def plot_predictor(ax):\n",
    "    xs = [xmin, xmax]\n",
    "    ys = [predict(w_best, x) for x in xs]\n",
    "    return ax.plot(xs, ys, c = \"black\")\n",
    "\n",
    "def plot_predictor2(ax):\n",
    "    xs = [xmin, xmax]\n",
    "    ys = [predict(w_gbest, x) for x in xs]\n",
    "    return ax.plot(xs, ys, c = \"blue\")\n",
    "    \n",
    "\n",
    "plt.ioff()\n",
    "\n",
    "fig = plt.figure(figsize=(5,5));\n",
    "ax = plt.axes();\n",
    "plot_Dtrain(ax)\n",
    "plot_predictor(ax)\n",
    "plot_predictor2(ax)\n",
    "ax.set_xlabel(\"$x$\", size=20)\n",
    "ax.set_xlim((xmin, xmax + 1))\n",
    "ax.set_ylabel(\"$y$\", size=20)\n",
    "ax.set_ylim((ymin, ymax + 1))\n",
    "ax.legend()\n",
    "ax.set_title(\"Predicción equitativa (azul) vs mínimos cuadrados (negro)\")\n",
    "fig.canvas.header_visible = False\n",
    "\n",
    "display(fig.canvas);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
