{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507068f8-5137-4042-9127-9f9a729203ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c972ee99-c564-4335-a52e-ded9716a8c5b",
   "metadata": {},
   "source": [
    "# Descenso de gradiente\n",
    "\n",
    "Hasta ahora, hemos visto el descenso de gradiente como un algoritmo de propósito general para optimizar la pérdida de entrenamiento.\n",
    "\n",
    "$$\\text{TrainLoss}(\\mathbf{w}) =  \\frac{1}{|\\mathcal{D}_{\\text{train}}|} \\sum_{(x, y)\\in\\mathcal{D}_{\\text{train}}} \\text{Loss}(x, y, \\mathbf{w})$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4239481b-ff93-4ca0-8290-d280f371ad07",
   "metadata": {},
   "source": [
    "1. Inicializa $\\mathbf{w} = [0, \\dots, 0]$\n",
    "2. Para $t = 1, \\dots, T$:\n",
    "    1. $\\mathbf{w} \\gets \\mathbf{w} - \\eta \\nabla_{\\mathbf{w}} \\mathrm{TrainLoss}(\\mathbf{w})$\n",
    "\n",
    "Un problema con el descenso de gradiente es que es lento: ¡Cada iteración requiere revisar todos los ejemplos de entrenamiento!\n",
    "\n",
    "La pérdida de entrenamiento considera **todos** los ejemplos del conjunto de datos de entrenamiento. Si tenemos un millón de ejemplos, el cálculo del gradiente requerirá iterar sobre todo este millón, y esto ocurre en cada época.\n",
    "\n",
    "¿Podemos progresar en nuestra búsqueda del mejor predictor antes de ver todos los datos?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a3e1f5-f364-4677-8d42-ef4bbcc854ec",
   "metadata": {},
   "source": [
    "# Descenso de gradiente estocástico\n",
    "\n",
    "En lugar de iterar sobre todos los ejemplos de entrenamiento para calcular un solo gradiente y avanzar un paso, el *descenso de gradiente estocástico* (SGD) itera por los ejemplos $(x, y)$ y actualiza los pesos $\\mathbf{w}$ con base en **cada** ejemplo.\n",
    "\n",
    "1. Inicializa $\\mathbf{w} = [0, \\dots, 0]$\n",
    "2. Para $t = 1, \\dots, T$:\n",
    "    1. Para $(x, y)\\in\\mathcal{D}_{\\text{train}}$:\n",
    "       1. $\\mathbf{w} \\gets \\mathbf{w} - \\eta \\nabla_{\\mathbf{w}} \\mathrm{Loss}(x, y, \\mathbf{w})$\n",
    "      \n",
    "\n",
    "Cada actualización a los pesos no es tan buena porque solo estamos considerando un ejemplo en lugar de todos. ¡Pero de esta manera podemos realizar muchas mas actualizaciones!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77c5dff-205a-4761-9022-ee5f4c0be794",
   "metadata": {},
   "source": [
    "## Reflexiones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f18158-3973-42e1-aed3-7552d3bb501c",
   "metadata": {},
   "source": [
    "### Sobre la cantidad de ejemplos en la actualización\n",
    "\n",
    "Pensemos que SGD está en un extremo, donde cada actualización de pesos considera un ejemplo de entrenamiento, y que el GD normalito está en otro extremo, donde cada actualización de pesos considera todos los ejemplos de entrenamiento. Podemos encontrar algoritmos entre estos dos extremos considerando $m$ ejemplos de entrenamiento con $1 < m < |\\mathcal{D}_{\\text{train}}|$. A esta modificación se le suele llamar *Descenso de Gradiente Estocástico por minilotes*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686fd16c-9bbe-4b22-89b8-f4786ded69a9",
   "metadata": {},
   "source": [
    "### Sobre el orden de la iteración\n",
    "\n",
    "Otras formas de variar el SGD es cambiando el orden en que iteramos los ejemplos de entrenamiento.  Una manera sería por ejemplo, iterando en orden aleatorio $\\mathcal{D}_{\\text{train}}$ en cada época.\n",
    "\n",
    "¿Por qué esta modificación puede ser importante?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecfe851-72ca-4789-8f82-77f929f49ae5",
   "metadata": {},
   "source": [
    "### Sobre el tamaño de paso\n",
    "\n",
    "Entre más grande el tamaño de paso $\\eta$, podemos llegar a la pérdida de entrenamiento mínima más rápido, pero también podemos obtener resultados inestables y no poder navegar la curva apropiadamente.\n",
    "\n",
    "Entre más pequeño el tamaño de paso $\\eta$, obtenemos mayor estabilidad, pero llegaremos al mínimo lentamente. Cuando $\\eta = 0$, los pesos nunca son actualizados.\n",
    "\n",
    "Una estrategia que podemos considerar es fijar el tamaño de paso inicialmente en $1$ y luego decrementarlo conforme realizamos las actualizaciones. Entre más iteraciones realicemos, mas pequeño se vuelve $\\eta$. En general podemos considerar cualquier función decreciente sobre la cantidad de iteraciones que contenga la asociación $(1, 1)$, que representa que en la primera actualización se utiliza $\\eta = 1$. Un ejemplo sería usar la función,\n",
    "$$\\eta = 1 / \\sqrt{\\#\\text{ de actualización}}.$$\n",
    "\n",
    "Modificaciones más sofisticadas al tamaño de paso pueden realizarse al considerar los datos de entrenamiento para ajustar $\\eta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356eecd5-c7e0-49f1-8c17-43b5fe73ac0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feta(n):\n",
    "    return 1 / np.sqrt(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b38bf5-b2f9-4103-8260-388404401210",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plt.close()\n",
    "plt.ioff()\n",
    "\n",
    "updates = np.linspace(1, 100, 100)\n",
    "decfunc = feta(updates)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "ax.plot(updates, decfunc, label = \"$\\\\frac{1}{\\\\sqrt{\\\\#\\\\text{ de actualización}}}$\")\n",
    "ax.legend()\n",
    "\n",
    "ax.set_xlabel(\"actualizaciones\")\n",
    "ax.set_ylabel(\"$\\\\eta$\")\n",
    "ax.set_xlim((0, 100))\n",
    "ax.set_ylim((0, 1.0))\n",
    "\n",
    "\n",
    "fig.canvas.header_visible = False\n",
    "display(fig.canvas);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68db9ff-7983-4040-adc9-1f1db762e4a2",
   "metadata": {},
   "source": [
    "# Demostración"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d52287c-6689-4fd3-be92-53351a483e05",
   "metadata": {},
   "source": [
    "Primero generamos un conjunto de datos lo suficientemente grande como para que sea notable la mejora en velocidad de convergencia.\n",
    "\n",
    "Fijamos un modelo real de los datos que no sea utilizado por los algoritmos pero nos permita corroborar el modelo aprendido por el algoritmo de optimización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02a8b11-9c12-43e0-9f11-d2f013a9d32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trueW = np.array([1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6aa810-adad-47f5-bbfe-5aff29ae80b9",
   "metadata": {},
   "source": [
    "Ahora creamos una función para generar ejemplos de entrenamiento a partir de entradas aleatorias de acuerdo una distribución normal.  Las salidas son calculadas a partir de la entrada, los pesos reales y un ruido adicional también distribuido conforme a una distribución normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bc4c42-f498-4e2a-870a-0432a318b42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_example():\n",
    "    x = np.random.randn(len(trueW))\n",
    "    y = trueW.dot(x) + np.random.randn()\n",
    "    return (x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ce9a94-3e4f-48bb-831c-c0e01b592a23",
   "metadata": {},
   "source": [
    "Generamos un millón de ejemplos de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9a99f3-0103-4f9e-955f-fc1c713f82d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dtrain = [generate_example() for i in range(10**6)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313a3af1-14d1-4510-9664-3eef3c8fc7ad",
   "metadata": {},
   "source": [
    "Corroboremos la distribución de entradas en $\\mathcal{D}_{\\text{train}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8a7780-de20-48e0-abcb-9ba67ac9dce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "plt.ioff()\n",
    "\n",
    "fig, axs = plt.subplots(\n",
    "    len(trueW) + 1, 1,\n",
    "    figsize = (5, 10),\n",
    "    sharex = True,\n",
    "    tight_layout = True,\n",
    ")\n",
    "\n",
    "for i, ax in enumerate(axs[:-1]):\n",
    "    ax.hist([x[i] for x, _ in Dtrain], bins = 20, density = True, fc = \"black\")\n",
    "    ax.yaxis.set_major_formatter(mpl.ticker.PercentFormatter(xmax = 1))\n",
    "    ax.set_ylim((0, .75))\n",
    "\n",
    "axs[-1].hist([y for _, y in Dtrain], bins = 20, density = True, fc = \"green\")\n",
    "axs[-1].yaxis.set_major_formatter(mpl.ticker.PercentFormatter(xmax = 1))\n",
    "axs[-1].set_ylim((0, .75))\n",
    "\n",
    "\n",
    "fig.canvas.header_visible = False\n",
    "display(fig.canvas);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a7c4ce-da23-4927-b56b-9367d9e617bf",
   "metadata": {},
   "source": [
    "Preparamos las funciones auxiliares para experimentar con los algoritmos de optimización."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c12f8a4-4217-4289-bf42-241d5e9436ae",
   "metadata": {},
   "source": [
    "Vamos a trabajar con regresión, pero no incluímos el término bias $1$ en el vector de características."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc011b1-cea2-449b-9dc6-17975e123d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi(x):\n",
    "    return np.array(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc29f2d-fc8b-4457-9c9a-a74423a84ccd",
   "metadata": {},
   "source": [
    "El vector de pesos inicial será $[0, \\dots, 0]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5391c896-ac4f-48da-93c8-760390bffe06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights():\n",
    "    return np.zeros(len(trueW))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c117471-44ba-4744-bf73-c5688af0eace",
   "metadata": {},
   "source": [
    "La predicción se realiza calculando el producto punto entre $\\mathbf{w}$ y el vector de características."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae99ef38-0489-45b2-b748-0e77c4eddce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, x):\n",
    "    return w.dot(phi(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c5e238-92b8-4192-81c4-2c2e39f1598f",
   "metadata": {},
   "source": [
    "Utilizaremos la pérdida cuadrática.\n",
    "\n",
    "$$\\text{Loss}_2(x, y, \\mathbf{w}) = (\\underbrace{\\underbrace{\\mathbf{w}\\cdot\\phi(x)}_{\\text{predicción}} - y}_{\\text{residual}})^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df88597b-a26c-4450-90b7-0b3f32f35624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual(x, y, w):\n",
    "    return predict(w, x) - y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387114bb-c6ed-4c99-8cae-c5de75fd9249",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_2(x, y, w):\n",
    "    return residual(x, y, w) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63941e15-a34d-46ca-94f3-9401c181be93",
   "metadata": {},
   "source": [
    "A partir de un conjunto de datos de entrenamiento, una función de pérdida y un vector de pesos. La pérdida de entrenamiento se implementa como la pérdida media sobre el conjunto de datos.\n",
    "\n",
    "$$\\text{TrainLoss}(\\mathbf{w}) = \\frac{1}{|\\mathcal{D}_{\\text{train}}|} \\sum_{(x, y)\\in\\mathcal{D}_{\\text{train}}} \\text{Loss}(x, y, \\mathbf{w})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bd09cf-87e7-4634-97dc-920a53a949ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loss(Dtrain, loss, w):\n",
    "    examples = len(Dtrain)\n",
    "    total = sum(loss(x, y, w) for x, y in Dtrain)\n",
    "    return total / examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e53b139-ffec-4f89-8944-8683ed6de6c5",
   "metadata": {},
   "source": [
    "Implementemos el gradiente de la pérdida cuadrática con respecto al vector de pesos $\\mathbf{w}$.\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\nabla_{\\mathbf{w}}\\text{Loss}_2(x, y, \\mathbf{w}) &=\n",
    "\\nabla_{\\mathbf{w}} (\\mathbf{w}\\cdot\\phi(x) - y)^2 \\\\\n",
    "&= 2(\\mathbf{w}\\cdot\\phi(x) - y)\\nabla_{\\mathbf{w}}(\\mathbf{w}\\cdot\\phi(x) - y) \\\\\n",
    "&= 2(\\mathbf{w}\\cdot\\phi(x) - y)(\\nabla_{\\mathbf{w}}\\mathbf{w}\\cdot\\phi(x) - \\nabla_{\\mathbf{w}} y) \\\\\n",
    "&= 2(\\mathbf{w}\\cdot\\phi(x) - y)\\nabla_{\\mathbf{w}}\\mathbf{w}\\cdot\\phi(x) \\\\\n",
    "&= 2(\\mathbf{w}\\cdot\\phi(x) - y)\\phi(x)\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ce23f2-c719-464d-b4fa-c08c6591f5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_loss_2(x, y, w):\n",
    "    return 2 * (residual(x, y, w)) * phi(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ed2baf-6638-4469-9b30-ca6a1843fee2",
   "metadata": {},
   "source": [
    "A partir de un conjunto de datos de entrenamiento, el gradiente de una función de pérdida y un vector de pesos. El gradiente de la pérdida de entrenamiento se implementa como el promedio de los gradientes de pérdida sobre el conjunto de datos.\n",
    "\n",
    "$$\\nabla_{\\mathbf{w}}\\text{TrainLoss}(\\mathbf{w}) = \\frac{1}{|\\mathcal{D}_{\\text{train}}|} \\sum_{(x, y)\\in\\mathcal{D}_{\\text{train}}} \\nabla_{\\mathbf{w}}\\text{Loss}(x, y, \\mathbf{w})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250d12bb-e070-4a42-ae86-508d0cf42aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_train_loss(Dtrain, grad_loss, w):\n",
    "    examples = len(Dtrain)\n",
    "    total = sum(grad_loss(x, y, w) for x, y in Dtrain)\n",
    "    return total / examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb343bd5-129d-4716-aab1-3ecc340bb63a",
   "metadata": {},
   "source": [
    "El algoritmo de descenso de gradiente (GD) realiza $T$ épocas actualizando los pesos tomando en cuenta a todos los datos de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0422f9-4170-4259-b144-e1500769c711",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GD(Dtrain, loss, grad_loss, make_w = init_weights, eta = 0.1, T = 500):\n",
    "    w_hist = []\n",
    "    tl_hist = []\n",
    "    w = make_w()\n",
    "    for t in range(1, T + 1):\n",
    "        tl = train_loss(Dtrain, loss, w)\n",
    "        gtl = grad_train_loss(Dtrain, grad_loss, w)\n",
    "        w_hist.append(w)\n",
    "        tl_hist.append(tl)\n",
    "        w = w - eta * gtl\n",
    "    return w, (w_hist, tl_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b2335e-d401-44dd-b40d-a9b3930438f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "gd_w, (gd_ws, gd_tls) = GD(Dtrain, loss_2, grad_loss_2, T = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec0cbda-4636-42c7-ad6c-2b2083429187",
   "metadata": {},
   "source": [
    "El algoritmo de descenso de gradiente estocástico (SGD) realiza $T$ épocas actualizando los pesos por cada ejemplo uno por uno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c612572-1217-406a-9a17-c8386b881917",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(Dtrain, loss, grad_loss, make_w = init_weights, eta_func = feta, T = 500):\n",
    "    w_hist = []\n",
    "    tl_hist = []\n",
    "    w = make_w()\n",
    "    n = 1\n",
    "    for t in range(1, T + 1):\n",
    "        w_hist.append(w)\n",
    "        tl_hist.append(train_loss(Dtrain, loss, w))\n",
    "        for x, y in Dtrain:\n",
    "            l = loss(x, y, w)\n",
    "            gl = grad_loss(x, y, w)\n",
    "            eta = eta_func(n)\n",
    "            w = w - eta * gl\n",
    "            n += 1\n",
    "    return w, (w_hist, tl_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740de417-4cff-4261-b8d4-485a2740e7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "sgd_w, (sgd_ws, sgd_tls) = SGD(Dtrain, loss_2, grad_loss_2, T = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d64571c-d4ff-4b94-af77-6938edbd16b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "plt.ioff()\n",
    "\n",
    "fig = plt.figure(figsize=(8,5))\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "epochs = list(range(1, len(sgd_tls) + 1))\n",
    "ax.plot(epochs, gd_tls, label = \"GD\")\n",
    "ax.plot(epochs, sgd_tls, label = \"SGD\")\n",
    "ax.legend()\n",
    "ax.set_title(\"Historia de TrainLoss\")\n",
    "ax.set_xlabel(\"época\")\n",
    "ax.set_ylabel(\"TrainLoss\")\n",
    "ax.set_xlim((epochs[0], epochs[-1]))\n",
    "\n",
    "fig.canvas.header_visible = False\n",
    "display(fig.canvas);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
